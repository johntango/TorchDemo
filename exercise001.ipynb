{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = torch.tensor(7)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "Matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "T = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [0, 1, 2]]])\n",
    "T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = torch.rand(3, 3)\n",
    "M2 = torch.rand(10,10)\n",
    "T1 = torch.rand(3, 4, 4)\n",
    "T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_ones = torch.ones(5, 5)\n",
    "\n",
    "# Use torch.triu() to get the upper triangular matrix\n",
    "triangular_matrix = torch.triu(matrix_ones)\n",
    "\n",
    "triangular_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torch.tril() to get the lower triangular matrix\n",
    "triangular_matrix = torch.tril(matrix_ones)\n",
    "triangular_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from a full random matrix of 5x5 get the first element of row 1, the first two elements of row 2, the first three elements of row 3, and so on\n",
    "matrix = torch.rand(5, 5)\n",
    "print(matrix)\n",
    "result = matrix * triangular_matrix\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of ones \"like\" result, but with the ones on the lower triangular part\n",
    "# make lower triangle ones by usine torch.like\n",
    "lower_ones = torch.ones_like(result).tril()\n",
    "lower_ones\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "A = torch.ones(2,3)\n",
    "M = torch.matmul(A, A.T)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze removes all the dimensions that have a size of 1\n",
    "A = torch.tensor([[1, 2, 3]]).squeeze()\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute the dimensions of a tensor for a color image 10x10x3 with 3 channels\n",
    "image = torch.rand(10, 10, 3)\n",
    "print(image.shape)\n",
    "new_image = image.permute(2, 0, 1)  \n",
    "new_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing and slicing\n",
    "# create a 3x3 matrix with random values\n",
    "matrix = torch.rand(3, 3)\n",
    "print(matrix)\n",
    "# get the first row\n",
    "first_row = matrix[0, 0:2]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Classes https://www.geeksforgeeks.org/python-classes-and-objects/#\n",
    "class Dog:\n",
    "    # Class Attribute\n",
    "    species = 'mammal'\n",
    "    # Initializer / Instance Attributes\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "    # instance method\n",
    "    def description(self):\n",
    "        return \"{} is {} years old\".format(self.name, self.age)\n",
    "    # instance method\n",
    "    def speak(self, sound):\n",
    "        return \"{} says {}\".format(self.name, sound)\n",
    "    \n",
    "# Instantiate the Dog object\n",
    "mikey = Dog(\"Mikey\", 6)\n",
    "# call our instance methods\n",
    "print(mikey.description())\n",
    "print(mikey.speak(\"Gruff Gruff\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data for line y = 2x + 3\n",
    "import numpy as np\n",
    "import torch\n",
    "X_train = torch.tensor([[1, 1], [2, 1], [3, 1], [4, 1], [5, 1]], dtype=torch.float32)\n",
    "y_train = torch.tensor([[5], [7], [9], [11], [13]], dtype=torch.float32)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.2075\n",
      "Epoch [200/1000], Loss: 0.0585\n",
      "Epoch [300/1000], Loss: 0.0165\n",
      "Epoch [400/1000], Loss: 0.0047\n",
      "Epoch [500/1000], Loss: 0.0013\n",
      "Epoch [600/1000], Loss: 0.0004\n",
      "Epoch [700/1000], Loss: 0.0001\n",
      "Epoch [800/1000], Loss: 0.0000\n",
      "Epoch [900/1000], Loss: 0.0000\n",
      "Epoch [1000/1000], Loss: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[15.0024],\n",
       "        [17.0034]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the Neural Network\n",
    "class LinearRegressionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionNet, self).__init__()\n",
    "        # One fully connected layer (input_dim=2, output_dim=1)\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the network\n",
    "net = LinearRegressionNet()\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Train the Network\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = net(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the network\n",
    "test_data = torch.tensor([[6, 1], [7, 1]], dtype=torch.float32)\n",
    "predictions = net(test_data)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recode using weights as nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.weights = nn.Parameter(torch.tensor([1, 1], requires_grad=True, dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # CHECK THIS FOR CORRECTNESS\n",
    "        return (x * self.weights).sum(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([1., 1.]))])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_01 = LinearRegressionModel()\n",
    "list(model_01.parameters())\n",
    "model_01.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model predictive power use torch.inference_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.]])\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y_preds =  model_01(X_train)\n",
    "    print(y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "### define the params to be optimized and lr the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(params = model_01.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.000533123267814517\n",
      "Epoch: 1, Loss: 0.0005295381997711957\n",
      "Epoch: 2, Loss: 0.0005259484169073403\n",
      "Epoch: 3, Loss: 0.0005224148044362664\n",
      "Epoch: 4, Loss: 0.0005188776995055377\n",
      "Epoch: 5, Loss: 0.0005153750535100698\n",
      "Epoch: 6, Loss: 0.0005118901608511806\n",
      "Epoch: 7, Loss: 0.0005084351287223399\n",
      "Epoch: 8, Loss: 0.0005050087347626686\n",
      "Epoch: 9, Loss: 0.0005015895585529506\n",
      "Epoch: 10, Loss: 0.0004982058308087289\n",
      "Epoch: 11, Loss: 0.0004948460264131427\n",
      "Epoch: 12, Loss: 0.0004915059544146061\n",
      "Epoch: 13, Loss: 0.00048818904906511307\n",
      "Epoch: 14, Loss: 0.0004849030519835651\n",
      "Epoch: 15, Loss: 0.00048161851009353995\n",
      "Epoch: 16, Loss: 0.0004783787881024182\n",
      "Epoch: 17, Loss: 0.00047514549805782735\n",
      "Epoch: 18, Loss: 0.00047193473437801003\n",
      "Epoch: 19, Loss: 0.0004687482141889632\n",
      "Epoch: 20, Loss: 0.0004655787197407335\n",
      "Epoch: 21, Loss: 0.00046244956320151687\n",
      "Epoch: 22, Loss: 0.0004593218327499926\n",
      "Epoch: 23, Loss: 0.00045622707693837583\n",
      "Epoch: 24, Loss: 0.00045314175076782703\n",
      "Epoch: 25, Loss: 0.0004500736831687391\n",
      "Epoch: 26, Loss: 0.00044704071478918195\n",
      "Epoch: 27, Loss: 0.00044402264757081866\n",
      "Epoch: 28, Loss: 0.0004410248657222837\n",
      "Epoch: 29, Loss: 0.0004380411992315203\n",
      "Epoch: 30, Loss: 0.0004350946401245892\n",
      "Epoch: 31, Loss: 0.00043215183541178703\n",
      "Epoch: 32, Loss: 0.00042922989814542234\n",
      "Epoch: 33, Loss: 0.00042634038254618645\n",
      "Epoch: 34, Loss: 0.0004234667867422104\n",
      "Epoch: 35, Loss: 0.00042060381383635104\n",
      "Epoch: 36, Loss: 0.0004177649097982794\n",
      "Epoch: 37, Loss: 0.00041494457400403917\n",
      "Epoch: 38, Loss: 0.00041214507655240595\n",
      "Epoch: 39, Loss: 0.00040936990990303457\n",
      "Epoch: 40, Loss: 0.0004065933171659708\n",
      "Epoch: 41, Loss: 0.000403855083277449\n",
      "Epoch: 42, Loss: 0.00040112753049470484\n",
      "Epoch: 43, Loss: 0.0003984219511039555\n",
      "Epoch: 44, Loss: 0.00039572533569298685\n",
      "Epoch: 45, Loss: 0.0003930672537535429\n",
      "Epoch: 46, Loss: 0.0003904122859239578\n",
      "Epoch: 47, Loss: 0.0003877799026668072\n",
      "Epoch: 48, Loss: 0.0003851472574751824\n",
      "Epoch: 49, Loss: 0.0003825576277449727\n",
      "Epoch: 50, Loss: 0.0003799683472607285\n",
      "Epoch: 51, Loss: 0.00037740811239928007\n",
      "Epoch: 52, Loss: 0.0003748509625438601\n",
      "Epoch: 53, Loss: 0.0003723301342688501\n",
      "Epoch: 54, Loss: 0.00036982158781029284\n",
      "Epoch: 55, Loss: 0.000367312750313431\n",
      "Epoch: 56, Loss: 0.00036483301664702594\n",
      "Epoch: 57, Loss: 0.0003623703378252685\n",
      "Epoch: 58, Loss: 0.0003599365591071546\n",
      "Epoch: 59, Loss: 0.00035749669768847525\n",
      "Epoch: 60, Loss: 0.00035508046858012676\n",
      "Epoch: 61, Loss: 0.0003526952932588756\n",
      "Epoch: 62, Loss: 0.0003503167536109686\n",
      "Epoch: 63, Loss: 0.00034793841768987477\n",
      "Epoch: 64, Loss: 0.0003455958212725818\n",
      "Epoch: 65, Loss: 0.00034326891181990504\n",
      "Epoch: 66, Loss: 0.0003409417113289237\n",
      "Epoch: 67, Loss: 0.00033864204306155443\n",
      "Epoch: 68, Loss: 0.00033635058207437396\n",
      "Epoch: 69, Loss: 0.0003340804541949183\n",
      "Epoch: 70, Loss: 0.00033182388870045543\n",
      "Epoch: 71, Loss: 0.0003295887145213783\n",
      "Epoch: 72, Loss: 0.00032736631692387164\n",
      "Epoch: 73, Loss: 0.00032515820930711925\n",
      "Epoch: 74, Loss: 0.00032295938581228256\n",
      "Epoch: 75, Loss: 0.00032078352523967624\n",
      "Epoch: 76, Loss: 0.0003186265239492059\n",
      "Epoch: 77, Loss: 0.00031646847492083907\n",
      "Epoch: 78, Loss: 0.0003143414796795696\n",
      "Epoch: 79, Loss: 0.0003122133784927428\n",
      "Epoch: 80, Loss: 0.0003101038164459169\n",
      "Epoch: 81, Loss: 0.0003080166643485427\n",
      "Epoch: 82, Loss: 0.0003059383016079664\n",
      "Epoch: 83, Loss: 0.000303873501252383\n",
      "Epoch: 84, Loss: 0.00030182074988260865\n",
      "Epoch: 85, Loss: 0.0002997879928443581\n",
      "Epoch: 86, Loss: 0.00029776367591694\n",
      "Epoch: 87, Loss: 0.0002957506221719086\n",
      "Epoch: 88, Loss: 0.0002937565150205046\n",
      "Epoch: 89, Loss: 0.00029176563839428127\n",
      "Epoch: 90, Loss: 0.00028980037313885987\n",
      "Epoch: 91, Loss: 0.0002878451196011156\n",
      "Epoch: 92, Loss: 0.0002859003434423357\n",
      "Epoch: 93, Loss: 0.0002839693333953619\n",
      "Epoch: 94, Loss: 0.0002820535155478865\n",
      "Epoch: 95, Loss: 0.00028014270355924964\n",
      "Epoch: 96, Loss: 0.00027826172299683094\n",
      "Epoch: 97, Loss: 0.0002763860684353858\n",
      "Epoch: 98, Loss: 0.0002745100646279752\n",
      "Epoch: 99, Loss: 0.00027266208780929446\n",
      "Epoch: 100, Loss: 0.00027081556618213654\n",
      "Epoch: 101, Loss: 0.0002689924440346658\n",
      "Epoch: 102, Loss: 0.00026717581204138696\n",
      "Epoch: 103, Loss: 0.00026537274243310094\n",
      "Epoch: 104, Loss: 0.0002635785494931042\n",
      "Epoch: 105, Loss: 0.00026180638815276325\n",
      "Epoch: 106, Loss: 0.00026004022220149636\n",
      "Epoch: 107, Loss: 0.00025828517391346395\n",
      "Epoch: 108, Loss: 0.0002565366739872843\n",
      "Epoch: 109, Loss: 0.0002548102056607604\n",
      "Epoch: 110, Loss: 0.0002530864148866385\n",
      "Epoch: 111, Loss: 0.0002513756917323917\n",
      "Epoch: 112, Loss: 0.00024967934587039053\n",
      "Epoch: 113, Loss: 0.0002479880640748888\n",
      "Epoch: 114, Loss: 0.00024632184067741036\n",
      "Epoch: 115, Loss: 0.0002446557045914233\n",
      "Epoch: 116, Loss: 0.00024300385848619044\n",
      "Epoch: 117, Loss: 0.00024136628780979663\n",
      "Epoch: 118, Loss: 0.00023973602219484746\n",
      "Epoch: 119, Loss: 0.00023811569553799927\n",
      "Epoch: 120, Loss: 0.00023651479568798095\n",
      "Epoch: 121, Loss: 0.00023491168394684792\n",
      "Epoch: 122, Loss: 0.00023332959972321987\n",
      "Epoch: 123, Loss: 0.0002317577600479126\n",
      "Epoch: 124, Loss: 0.00023018373758532107\n",
      "Epoch: 125, Loss: 0.00022864369384478778\n",
      "Epoch: 126, Loss: 0.0002270956028951332\n",
      "Epoch: 127, Loss: 0.00022555282339453697\n",
      "Epoch: 128, Loss: 0.00022403671755455434\n",
      "Epoch: 129, Loss: 0.0002225213684141636\n",
      "Epoch: 130, Loss: 0.00022102468938101083\n",
      "Epoch: 131, Loss: 0.00021953173563815653\n",
      "Epoch: 132, Loss: 0.00021804981224704534\n",
      "Epoch: 133, Loss: 0.00021657932666130364\n",
      "Epoch: 134, Loss: 0.00021511700470000505\n",
      "Epoch: 135, Loss: 0.00021366406872402877\n",
      "Epoch: 136, Loss: 0.0002122195000993088\n",
      "Epoch: 137, Loss: 0.00021079115686006844\n",
      "Epoch: 138, Loss: 0.00020936803775839508\n",
      "Epoch: 139, Loss: 0.00020795692398678511\n",
      "Epoch: 140, Loss: 0.00020655053958762437\n",
      "Epoch: 141, Loss: 0.00020515960932243615\n",
      "Epoch: 142, Loss: 0.00020377345208544284\n",
      "Epoch: 143, Loss: 0.00020239746663719416\n",
      "Epoch: 144, Loss: 0.00020102618145756423\n",
      "Epoch: 145, Loss: 0.00019967097614426166\n",
      "Epoch: 146, Loss: 0.00019832016550935805\n",
      "Epoch: 147, Loss: 0.0001969845761777833\n",
      "Epoch: 148, Loss: 0.0001956471533048898\n",
      "Epoch: 149, Loss: 0.00019432941917330027\n",
      "Epoch: 150, Loss: 0.00019302342843730003\n",
      "Epoch: 151, Loss: 0.00019171781605109572\n",
      "Epoch: 152, Loss: 0.00019042509666178375\n",
      "Epoch: 153, Loss: 0.00018913880921900272\n",
      "Epoch: 154, Loss: 0.0001878610346466303\n",
      "Epoch: 155, Loss: 0.00018659296620171517\n",
      "Epoch: 156, Loss: 0.00018533863476477563\n",
      "Epoch: 157, Loss: 0.00018408385221846402\n",
      "Epoch: 158, Loss: 0.00018283643294125795\n",
      "Epoch: 159, Loss: 0.00018160576291847974\n",
      "Epoch: 160, Loss: 0.00018037710106000304\n",
      "Epoch: 161, Loss: 0.0001791652466636151\n",
      "Epoch: 162, Loss: 0.00017795471649151295\n",
      "Epoch: 163, Loss: 0.00017675429990049452\n",
      "Epoch: 164, Loss: 0.00017555980593897402\n",
      "Epoch: 165, Loss: 0.00017438323993701488\n",
      "Epoch: 166, Loss: 0.00017319605103693902\n",
      "Epoch: 167, Loss: 0.00017203131574206054\n",
      "Epoch: 168, Loss: 0.00017087349260691553\n",
      "Epoch: 169, Loss: 0.00016971526201814413\n",
      "Epoch: 170, Loss: 0.0001685745664872229\n",
      "Epoch: 171, Loss: 0.0001674352097325027\n",
      "Epoch: 172, Loss: 0.00016630096070002764\n",
      "Epoch: 173, Loss: 0.00016517347830813378\n",
      "Epoch: 174, Loss: 0.0001640624541323632\n",
      "Epoch: 175, Loss: 0.00016294927627313882\n",
      "Epoch: 176, Loss: 0.00016185521963052452\n",
      "Epoch: 177, Loss: 0.00016076237079687417\n",
      "Epoch: 178, Loss: 0.0001596736692590639\n",
      "Epoch: 179, Loss: 0.00015860592247918248\n",
      "Epoch: 180, Loss: 0.00015752456965856254\n",
      "Epoch: 181, Loss: 0.000156465481268242\n",
      "Epoch: 182, Loss: 0.00015541238826699555\n",
      "Epoch: 183, Loss: 0.00015436105604749173\n",
      "Epoch: 184, Loss: 0.0001533187460154295\n",
      "Epoch: 185, Loss: 0.00015228556003421545\n",
      "Epoch: 186, Loss: 0.0001512558665126562\n",
      "Epoch: 187, Loss: 0.00015023445303086191\n",
      "Epoch: 188, Loss: 0.0001492183655500412\n",
      "Epoch: 189, Loss: 0.00014821250806562603\n",
      "Epoch: 190, Loss: 0.0001472135481890291\n",
      "Epoch: 191, Loss: 0.00014621797890868038\n",
      "Epoch: 192, Loss: 0.00014522927813231945\n",
      "Epoch: 193, Loss: 0.00014425060362555087\n",
      "Epoch: 194, Loss: 0.0001432770659448579\n",
      "Epoch: 195, Loss: 0.0001423114736098796\n",
      "Epoch: 196, Loss: 0.00014135203673504293\n",
      "Epoch: 197, Loss: 0.0001403903734171763\n",
      "Epoch: 198, Loss: 0.00013944362581241876\n",
      "Epoch: 199, Loss: 0.00013850894174538553\n",
      "Epoch: 200, Loss: 0.0001375738938804716\n",
      "Epoch: 201, Loss: 0.00013664166908711195\n",
      "Epoch: 202, Loss: 0.0001357178989565\n",
      "Epoch: 203, Loss: 0.00013480294728651643\n",
      "Epoch: 204, Loss: 0.00013388885417953134\n",
      "Epoch: 205, Loss: 0.00013299175770953298\n",
      "Epoch: 206, Loss: 0.00013208978634793311\n",
      "Epoch: 207, Loss: 0.00013119919458404183\n",
      "Epoch: 208, Loss: 0.00013030966510996222\n",
      "Epoch: 209, Loss: 0.00012943436740897596\n",
      "Epoch: 210, Loss: 0.0001285634789383039\n",
      "Epoch: 211, Loss: 0.00012769624299835414\n",
      "Epoch: 212, Loss: 0.00012683341628871858\n",
      "Epoch: 213, Loss: 0.00012597389286383986\n",
      "Epoch: 214, Loss: 0.00012512384273577482\n",
      "Epoch: 215, Loss: 0.00012427558249328285\n",
      "Epoch: 216, Loss: 0.00012343877460807562\n",
      "Epoch: 217, Loss: 0.00012260829680599272\n",
      "Epoch: 218, Loss: 0.00012177962344139814\n",
      "Epoch: 219, Loss: 0.00012095846614101902\n",
      "Epoch: 220, Loss: 0.0001201423438033089\n",
      "Epoch: 221, Loss: 0.00011932912457268685\n",
      "Epoch: 222, Loss: 0.00011851949238916859\n",
      "Epoch: 223, Loss: 0.00011772330617532134\n",
      "Epoch: 224, Loss: 0.00011692421685438603\n",
      "Epoch: 225, Loss: 0.00011613821698119864\n",
      "Epoch: 226, Loss: 0.00011536042438820004\n",
      "Epoch: 227, Loss: 0.00011457500659162179\n",
      "Epoch: 228, Loss: 0.00011380341311451048\n",
      "Epoch: 229, Loss: 0.00011303545034024864\n",
      "Epoch: 230, Loss: 0.00011227213690290228\n",
      "Epoch: 231, Loss: 0.00011151699436595663\n",
      "Epoch: 232, Loss: 0.00011076172813773155\n",
      "Epoch: 233, Loss: 0.000110013505036477\n",
      "Epoch: 234, Loss: 0.00010926694085355848\n",
      "Epoch: 235, Loss: 0.00010853381536435336\n",
      "Epoch: 236, Loss: 0.00010779680451378226\n",
      "Epoch: 237, Loss: 0.00010707405454013497\n",
      "Epoch: 238, Loss: 0.00010635117359925061\n",
      "Epoch: 239, Loss: 0.00010563583055045456\n",
      "Epoch: 240, Loss: 0.00010491839202586561\n",
      "Epoch: 241, Loss: 0.00010421371553093195\n",
      "Epoch: 242, Loss: 0.00010350639786338434\n",
      "Epoch: 243, Loss: 0.00010280584683641791\n",
      "Epoch: 244, Loss: 0.00010211354674538597\n",
      "Epoch: 245, Loss: 0.00010142786049982533\n",
      "Epoch: 246, Loss: 0.00010074061719933525\n",
      "Epoch: 247, Loss: 0.0001000634947558865\n",
      "Epoch: 248, Loss: 9.938899893313646e-05\n",
      "Epoch: 249, Loss: 9.871998190646991e-05\n",
      "Epoch: 250, Loss: 9.80491386144422e-05\n",
      "Epoch: 251, Loss: 9.738982771523297e-05\n",
      "Epoch: 252, Loss: 9.672759915702045e-05\n",
      "Epoch: 253, Loss: 9.60772595135495e-05\n",
      "Epoch: 254, Loss: 9.54274510149844e-05\n",
      "Epoch: 255, Loss: 9.47869339142926e-05\n",
      "Epoch: 256, Loss: 9.414376108907163e-05\n",
      "Epoch: 257, Loss: 9.350657637696713e-05\n",
      "Epoch: 258, Loss: 9.287944703828543e-05\n",
      "Epoch: 259, Loss: 9.22504550544545e-05\n",
      "Epoch: 260, Loss: 9.162848436972126e-05\n",
      "Epoch: 261, Loss: 9.100893657887354e-05\n",
      "Epoch: 262, Loss: 9.03919426491484e-05\n",
      "Epoch: 263, Loss: 8.978520781965926e-05\n",
      "Epoch: 264, Loss: 8.91795934876427e-05\n",
      "Epoch: 265, Loss: 8.857422653818503e-05\n",
      "Epoch: 266, Loss: 8.798253838904202e-05\n",
      "Epoch: 267, Loss: 8.738708856981248e-05\n",
      "Epoch: 268, Loss: 8.679265738464892e-05\n",
      "Epoch: 269, Loss: 8.620700828032568e-05\n",
      "Epoch: 270, Loss: 8.562462608097121e-05\n",
      "Epoch: 271, Loss: 8.504358265781775e-05\n",
      "Epoch: 272, Loss: 8.447134314337745e-05\n",
      "Epoch: 273, Loss: 8.390316361328587e-05\n",
      "Epoch: 274, Loss: 8.333772711921483e-05\n",
      "Epoch: 275, Loss: 8.2778962678276e-05\n",
      "Epoch: 276, Loss: 8.221765165217221e-05\n",
      "Epoch: 277, Loss: 8.166527550201863e-05\n",
      "Epoch: 278, Loss: 8.1110410974361e-05\n",
      "Epoch: 279, Loss: 8.05629970273003e-05\n",
      "Epoch: 280, Loss: 8.002563117770478e-05\n",
      "Epoch: 281, Loss: 7.948385609779507e-05\n",
      "Epoch: 282, Loss: 7.894723967183381e-05\n",
      "Epoch: 283, Loss: 7.841552724130452e-05\n",
      "Epoch: 284, Loss: 7.788823859300464e-05\n",
      "Epoch: 285, Loss: 7.736525731161237e-05\n",
      "Epoch: 286, Loss: 7.6843258284498e-05\n",
      "Epoch: 287, Loss: 7.631898915860802e-05\n",
      "Epoch: 288, Loss: 7.580761302961037e-05\n",
      "Epoch: 289, Loss: 7.529651338700205e-05\n",
      "Epoch: 290, Loss: 7.478886982426047e-05\n",
      "Epoch: 291, Loss: 7.428465323755518e-05\n",
      "Epoch: 292, Loss: 7.37816808396019e-05\n",
      "Epoch: 293, Loss: 7.32829503249377e-05\n",
      "Epoch: 294, Loss: 7.278937118826434e-05\n",
      "Epoch: 295, Loss: 7.229271432152018e-05\n",
      "Epoch: 296, Loss: 7.180744432844222e-05\n",
      "Epoch: 297, Loss: 7.132549217203632e-05\n",
      "Epoch: 298, Loss: 7.083855598466471e-05\n",
      "Epoch: 299, Loss: 7.036164606688544e-05\n",
      "Epoch: 300, Loss: 6.98841831763275e-05\n",
      "Epoch: 301, Loss: 6.941417086636648e-05\n",
      "Epoch: 302, Loss: 6.89449516357854e-05\n",
      "Epoch: 303, Loss: 6.847937038401142e-05\n",
      "Epoch: 304, Loss: 6.801553536206484e-05\n",
      "Epoch: 305, Loss: 6.755821232218295e-05\n",
      "Epoch: 306, Loss: 6.710302841383964e-05\n",
      "Epoch: 307, Loss: 6.665276305284351e-05\n",
      "Epoch: 308, Loss: 6.620143540203571e-05\n",
      "Epoch: 309, Loss: 6.575511361006647e-05\n",
      "Epoch: 310, Loss: 6.531323015224189e-05\n",
      "Epoch: 311, Loss: 6.486725033028051e-05\n",
      "Epoch: 312, Loss: 6.443080201279372e-05\n",
      "Epoch: 313, Loss: 6.399405538104475e-05\n",
      "Epoch: 314, Loss: 6.35636824881658e-05\n",
      "Epoch: 315, Loss: 6.313165795290843e-05\n",
      "Epoch: 316, Loss: 6.270953599596396e-05\n",
      "Epoch: 317, Loss: 6.228470738278702e-05\n",
      "Epoch: 318, Loss: 6.186180689837784e-05\n",
      "Epoch: 319, Loss: 6.144838698673993e-05\n",
      "Epoch: 320, Loss: 6.103194391471334e-05\n",
      "Epoch: 321, Loss: 6.0618251154664904e-05\n",
      "Epoch: 322, Loss: 6.021082663210109e-05\n",
      "Epoch: 323, Loss: 5.9803762269439176e-05\n",
      "Epoch: 324, Loss: 5.9400197642389685e-05\n",
      "Epoch: 325, Loss: 5.899800453335047e-05\n",
      "Epoch: 326, Loss: 5.860159217263572e-05\n",
      "Epoch: 327, Loss: 5.8208988775731996e-05\n",
      "Epoch: 328, Loss: 5.78110630158335e-05\n",
      "Epoch: 329, Loss: 5.742518260376528e-05\n",
      "Epoch: 330, Loss: 5.703223723685369e-05\n",
      "Epoch: 331, Loss: 5.6649230828043073e-05\n",
      "Epoch: 332, Loss: 5.626782149192877e-05\n",
      "Epoch: 333, Loss: 5.5888482165755704e-05\n",
      "Epoch: 334, Loss: 5.551154026761651e-05\n",
      "Epoch: 335, Loss: 5.513654832611792e-05\n",
      "Epoch: 336, Loss: 5.4762826039223e-05\n",
      "Epoch: 337, Loss: 5.43928072147537e-05\n",
      "Epoch: 338, Loss: 5.4024749260861427e-05\n",
      "Epoch: 339, Loss: 5.366035475162789e-05\n",
      "Epoch: 340, Loss: 5.329889972927049e-05\n",
      "Epoch: 341, Loss: 5.2937619329895824e-05\n",
      "Epoch: 342, Loss: 5.2582461648853496e-05\n",
      "Epoch: 343, Loss: 5.222690379014239e-05\n",
      "Epoch: 344, Loss: 5.187472561374307e-05\n",
      "Epoch: 345, Loss: 5.152547964826226e-05\n",
      "Epoch: 346, Loss: 5.117790351505391e-05\n",
      "Epoch: 347, Loss: 5.0831738917622715e-05\n",
      "Epoch: 348, Loss: 5.0489092245697975e-05\n",
      "Epoch: 349, Loss: 5.0146983994636685e-05\n",
      "Epoch: 350, Loss: 4.980763333151117e-05\n",
      "Epoch: 351, Loss: 4.9472924729343504e-05\n",
      "Epoch: 352, Loss: 4.913957673124969e-05\n",
      "Epoch: 353, Loss: 4.880774940829724e-05\n",
      "Epoch: 354, Loss: 4.847867239732295e-05\n",
      "Epoch: 355, Loss: 4.815232750843279e-05\n",
      "Epoch: 356, Loss: 4.782546602655202e-05\n",
      "Epoch: 357, Loss: 4.7501329390797764e-05\n",
      "Epoch: 358, Loss: 4.71822677354794e-05\n",
      "Epoch: 359, Loss: 4.686418833443895e-05\n",
      "Epoch: 360, Loss: 4.654901204048656e-05\n",
      "Epoch: 361, Loss: 4.623274799087085e-05\n",
      "Epoch: 362, Loss: 4.592044570017606e-05\n",
      "Epoch: 363, Loss: 4.561032255878672e-05\n",
      "Epoch: 364, Loss: 4.530143633019179e-05\n",
      "Epoch: 365, Loss: 4.4996922952122986e-05\n",
      "Epoch: 366, Loss: 4.469361374503933e-05\n",
      "Epoch: 367, Loss: 4.439463373273611e-05\n",
      "Epoch: 368, Loss: 4.409304528962821e-05\n",
      "Epoch: 369, Loss: 4.379508754936978e-05\n",
      "Epoch: 370, Loss: 4.349913069745526e-05\n",
      "Epoch: 371, Loss: 4.320761581766419e-05\n",
      "Epoch: 372, Loss: 4.2914187361020595e-05\n",
      "Epoch: 373, Loss: 4.262478250893764e-05\n",
      "Epoch: 374, Loss: 4.233845538692549e-05\n",
      "Epoch: 375, Loss: 4.204974538879469e-05\n",
      "Epoch: 376, Loss: 4.17684277635999e-05\n",
      "Epoch: 377, Loss: 4.148672451265156e-05\n",
      "Epoch: 378, Loss: 4.1206196328857914e-05\n",
      "Epoch: 379, Loss: 4.092515882803127e-05\n",
      "Epoch: 380, Loss: 4.0650793380336836e-05\n",
      "Epoch: 381, Loss: 4.037782855448313e-05\n",
      "Epoch: 382, Loss: 4.010316479252651e-05\n",
      "Epoch: 383, Loss: 3.98298361687921e-05\n",
      "Epoch: 384, Loss: 3.9561946323374286e-05\n",
      "Epoch: 385, Loss: 3.929671584046446e-05\n",
      "Epoch: 386, Loss: 3.903117612935603e-05\n",
      "Epoch: 387, Loss: 3.87672298529651e-05\n",
      "Epoch: 388, Loss: 3.8506048440467566e-05\n",
      "Epoch: 389, Loss: 3.824439409072511e-05\n",
      "Epoch: 390, Loss: 3.798634861595929e-05\n",
      "Epoch: 391, Loss: 3.7730605981778353e-05\n",
      "Epoch: 392, Loss: 3.747770460904576e-05\n",
      "Epoch: 393, Loss: 3.7226203858153895e-05\n",
      "Epoch: 394, Loss: 3.6974797694711015e-05\n",
      "Epoch: 395, Loss: 3.672444654512219e-05\n",
      "Epoch: 396, Loss: 3.64752413588576e-05\n",
      "Epoch: 397, Loss: 3.6228535464033484e-05\n",
      "Epoch: 398, Loss: 3.5987046430818737e-05\n",
      "Epoch: 399, Loss: 3.574319271137938e-05\n",
      "Epoch: 400, Loss: 3.550070687197149e-05\n",
      "Epoch: 401, Loss: 3.526112777763046e-05\n",
      "Epoch: 402, Loss: 3.502487743389793e-05\n",
      "Epoch: 403, Loss: 3.478674989310093e-05\n",
      "Epoch: 404, Loss: 3.4551616408862174e-05\n",
      "Epoch: 405, Loss: 3.4319335100008175e-05\n",
      "Epoch: 406, Loss: 3.408843622310087e-05\n",
      "Epoch: 407, Loss: 3.3857162634376436e-05\n",
      "Epoch: 408, Loss: 3.362842107890174e-05\n",
      "Epoch: 409, Loss: 3.340181865496561e-05\n",
      "Epoch: 410, Loss: 3.3174866985064e-05\n",
      "Epoch: 411, Loss: 3.2953863410511985e-05\n",
      "Epoch: 412, Loss: 3.272985486546531e-05\n",
      "Epoch: 413, Loss: 3.250892768846825e-05\n",
      "Epoch: 414, Loss: 3.228796776966192e-05\n",
      "Epoch: 415, Loss: 3.2071151508716866e-05\n",
      "Epoch: 416, Loss: 3.185300374752842e-05\n",
      "Epoch: 417, Loss: 3.163947621942498e-05\n",
      "Epoch: 418, Loss: 3.142461719107814e-05\n",
      "Epoch: 419, Loss: 3.121382906101644e-05\n",
      "Epoch: 420, Loss: 3.100299363723025e-05\n",
      "Epoch: 421, Loss: 3.079511952819303e-05\n",
      "Epoch: 422, Loss: 3.058505535591394e-05\n",
      "Epoch: 423, Loss: 3.0377905204659328e-05\n",
      "Epoch: 424, Loss: 3.0174862331477925e-05\n",
      "Epoch: 425, Loss: 2.996979128511157e-05\n",
      "Epoch: 426, Loss: 2.9766690204269253e-05\n",
      "Epoch: 427, Loss: 2.9568542231572792e-05\n",
      "Epoch: 428, Loss: 2.9368191462708637e-05\n",
      "Epoch: 429, Loss: 2.917138408520259e-05\n",
      "Epoch: 430, Loss: 2.8972566724405624e-05\n",
      "Epoch: 431, Loss: 2.8777267289115116e-05\n",
      "Epoch: 432, Loss: 2.858399602700956e-05\n",
      "Epoch: 433, Loss: 2.8387710699462332e-05\n",
      "Epoch: 434, Loss: 2.8196960556670092e-05\n",
      "Epoch: 435, Loss: 2.800965194182936e-05\n",
      "Epoch: 436, Loss: 2.7819047318189405e-05\n",
      "Epoch: 437, Loss: 2.762945950962603e-05\n",
      "Epoch: 438, Loss: 2.744282755884342e-05\n",
      "Epoch: 439, Loss: 2.7259948183200322e-05\n",
      "Epoch: 440, Loss: 2.7073319870396517e-05\n",
      "Epoch: 441, Loss: 2.6890425942838192e-05\n",
      "Epoch: 442, Loss: 2.6711466489359736e-05\n",
      "Epoch: 443, Loss: 2.6528559828875586e-05\n",
      "Epoch: 444, Loss: 2.6351550332037732e-05\n",
      "Epoch: 445, Loss: 2.617362406454049e-05\n",
      "Epoch: 446, Loss: 2.599530853331089e-05\n",
      "Epoch: 447, Loss: 2.5823901523835957e-05\n",
      "Epoch: 448, Loss: 2.5647308575571515e-05\n",
      "Epoch: 449, Loss: 2.5474586436757818e-05\n",
      "Epoch: 450, Loss: 2.5302029825979844e-05\n",
      "Epoch: 451, Loss: 2.513103208912071e-05\n",
      "Epoch: 452, Loss: 2.496177694411017e-05\n",
      "Epoch: 453, Loss: 2.479357317497488e-05\n",
      "Epoch: 454, Loss: 2.462474185449537e-05\n",
      "Epoch: 455, Loss: 2.4460507120238617e-05\n",
      "Epoch: 456, Loss: 2.4297323761857115e-05\n",
      "Epoch: 457, Loss: 2.4130637029884383e-05\n",
      "Epoch: 458, Loss: 2.3969671019585803e-05\n",
      "Epoch: 459, Loss: 2.3806529497960582e-05\n",
      "Epoch: 460, Loss: 2.3644246539333835e-05\n",
      "Epoch: 461, Loss: 2.3486316422349773e-05\n",
      "Epoch: 462, Loss: 2.3328480892814696e-05\n",
      "Epoch: 463, Loss: 2.316956306458451e-05\n",
      "Epoch: 464, Loss: 2.3014155885903165e-05\n",
      "Epoch: 465, Loss: 2.285946175106801e-05\n",
      "Epoch: 466, Loss: 2.2704181901644915e-05\n",
      "Epoch: 467, Loss: 2.2551455913344398e-05\n",
      "Epoch: 468, Loss: 2.2398782675736584e-05\n",
      "Epoch: 469, Loss: 2.2248641471378505e-05\n",
      "Epoch: 470, Loss: 2.2096815882832743e-05\n",
      "Epoch: 471, Loss: 2.1950434529571794e-05\n",
      "Epoch: 472, Loss: 2.1801324692205526e-05\n",
      "Epoch: 473, Loss: 2.1654101146850735e-05\n",
      "Epoch: 474, Loss: 2.1506897610379383e-05\n",
      "Epoch: 475, Loss: 2.1359785023378208e-05\n",
      "Epoch: 476, Loss: 2.121678517141845e-05\n",
      "Epoch: 477, Loss: 2.1072564777568914e-05\n",
      "Epoch: 478, Loss: 2.0932897314196452e-05\n",
      "Epoch: 479, Loss: 2.0788200345123187e-05\n",
      "Epoch: 480, Loss: 2.065006992779672e-05\n",
      "Epoch: 481, Loss: 2.050869443337433e-05\n",
      "Epoch: 482, Loss: 2.0373297957121395e-05\n",
      "Epoch: 483, Loss: 2.0233026589266956e-05\n",
      "Epoch: 484, Loss: 2.0096606021979824e-05\n",
      "Epoch: 485, Loss: 1.996168793994002e-05\n",
      "Epoch: 486, Loss: 1.982866342586931e-05\n",
      "Epoch: 487, Loss: 1.9693887225002982e-05\n",
      "Epoch: 488, Loss: 1.956032974703703e-05\n",
      "Epoch: 489, Loss: 1.9430104657658376e-05\n",
      "Epoch: 490, Loss: 1.9295724996482022e-05\n",
      "Epoch: 491, Loss: 1.9167413483955897e-05\n",
      "Epoch: 492, Loss: 1.9035865989280865e-05\n",
      "Epoch: 493, Loss: 1.8907372577814385e-05\n",
      "Epoch: 494, Loss: 1.8781229300657287e-05\n",
      "Epoch: 495, Loss: 1.8654462110134773e-05\n",
      "Epoch: 496, Loss: 1.8528124201111495e-05\n",
      "Epoch: 497, Loss: 1.8401178749627434e-05\n",
      "Epoch: 498, Loss: 1.8278209608979523e-05\n",
      "Epoch: 499, Loss: 1.8156231817556545e-05\n",
      "Epoch: 500, Loss: 1.803296800062526e-05\n",
      "Epoch: 501, Loss: 1.7909163943841122e-05\n",
      "Epoch: 502, Loss: 1.77910114871338e-05\n",
      "Epoch: 503, Loss: 1.7670672605163418e-05\n",
      "Epoch: 504, Loss: 1.7549071344546974e-05\n",
      "Epoch: 505, Loss: 1.7430904335924424e-05\n",
      "Epoch: 506, Loss: 1.7311933333985507e-05\n",
      "Epoch: 507, Loss: 1.7196169210365042e-05\n",
      "Epoch: 508, Loss: 1.7080619727494195e-05\n",
      "Epoch: 509, Loss: 1.6965175746008754e-05\n",
      "Epoch: 510, Loss: 1.6851312466314994e-05\n",
      "Epoch: 511, Loss: 1.6736930774641223e-05\n",
      "Epoch: 512, Loss: 1.6626192518742755e-05\n",
      "Epoch: 513, Loss: 1.6512354704900645e-05\n",
      "Epoch: 514, Loss: 1.640102345845662e-05\n",
      "Epoch: 515, Loss: 1.6289684936054982e-05\n",
      "Epoch: 516, Loss: 1.618021633476019e-05\n",
      "Epoch: 517, Loss: 1.606943624210544e-05\n",
      "Epoch: 518, Loss: 1.5961726603563875e-05\n",
      "Epoch: 519, Loss: 1.585553036420606e-05\n",
      "Epoch: 520, Loss: 1.5749035810586065e-05\n",
      "Epoch: 521, Loss: 1.5641237041563727e-05\n",
      "Epoch: 522, Loss: 1.553456786496099e-05\n",
      "Epoch: 523, Loss: 1.5429915947606787e-05\n",
      "Epoch: 524, Loss: 1.5326771972468123e-05\n",
      "Epoch: 525, Loss: 1.5222717593132984e-05\n",
      "Epoch: 526, Loss: 1.5120403077162337e-05\n",
      "Epoch: 527, Loss: 1.501718088547932e-05\n",
      "Epoch: 528, Loss: 1.4915827705408446e-05\n",
      "Epoch: 529, Loss: 1.4814577298238873e-05\n",
      "Epoch: 530, Loss: 1.4717766134708654e-05\n",
      "Epoch: 531, Loss: 1.4617166016250849e-05\n",
      "Epoch: 532, Loss: 1.4518773241434246e-05\n",
      "Epoch: 533, Loss: 1.4420713341678493e-05\n",
      "Epoch: 534, Loss: 1.4321890375867952e-05\n",
      "Epoch: 535, Loss: 1.4227584870241117e-05\n",
      "Epoch: 536, Loss: 1.413064182997914e-05\n",
      "Epoch: 537, Loss: 1.4034031664778013e-05\n",
      "Epoch: 538, Loss: 1.3940679309598636e-05\n",
      "Epoch: 539, Loss: 1.3844954082742333e-05\n",
      "Epoch: 540, Loss: 1.375379360979423e-05\n",
      "Epoch: 541, Loss: 1.3658836905960925e-05\n",
      "Epoch: 542, Loss: 1.3567478163167834e-05\n",
      "Epoch: 543, Loss: 1.3475442756316625e-05\n",
      "Epoch: 544, Loss: 1.3385177226155065e-05\n",
      "Epoch: 545, Loss: 1.3294114978634752e-05\n",
      "Epoch: 546, Loss: 1.3206008588895202e-05\n",
      "Epoch: 547, Loss: 1.3116082300257403e-05\n",
      "Epoch: 548, Loss: 1.302765122090932e-05\n",
      "Epoch: 549, Loss: 1.293959667236777e-05\n",
      "Epoch: 550, Loss: 1.2851841347583104e-05\n",
      "Epoch: 551, Loss: 1.276546208828222e-05\n",
      "Epoch: 552, Loss: 1.2678472558036447e-05\n",
      "Epoch: 553, Loss: 1.259357941307826e-05\n",
      "Epoch: 554, Loss: 1.2508974577940535e-05\n",
      "Epoch: 555, Loss: 1.2424001397448592e-05\n",
      "Epoch: 556, Loss: 1.2338760825514328e-05\n",
      "Epoch: 557, Loss: 1.2255573892616667e-05\n",
      "Epoch: 558, Loss: 1.2172786227893084e-05\n",
      "Epoch: 559, Loss: 1.2090950804122258e-05\n",
      "Epoch: 560, Loss: 1.2008722478640266e-05\n",
      "Epoch: 561, Loss: 1.1927890227525495e-05\n",
      "Epoch: 562, Loss: 1.1848350368381944e-05\n",
      "Epoch: 563, Loss: 1.176730620500166e-05\n",
      "Epoch: 564, Loss: 1.1689458006003406e-05\n",
      "Epoch: 565, Loss: 1.1611233276198618e-05\n",
      "Epoch: 566, Loss: 1.1532758435350843e-05\n",
      "Epoch: 567, Loss: 1.145387432188727e-05\n",
      "Epoch: 568, Loss: 1.137702383857686e-05\n",
      "Epoch: 569, Loss: 1.1299345715087838e-05\n",
      "Epoch: 570, Loss: 1.1223592082387768e-05\n",
      "Epoch: 571, Loss: 1.1147518307552673e-05\n",
      "Epoch: 572, Loss: 1.1072047527704854e-05\n",
      "Epoch: 573, Loss: 1.0997652680089232e-05\n",
      "Epoch: 574, Loss: 1.0923147783614695e-05\n",
      "Epoch: 575, Loss: 1.0848759302461985e-05\n",
      "Epoch: 576, Loss: 1.077697379514575e-05\n",
      "Epoch: 577, Loss: 1.0705742170102894e-05\n",
      "Epoch: 578, Loss: 1.0632098565110937e-05\n",
      "Epoch: 579, Loss: 1.0560250302660279e-05\n",
      "Epoch: 580, Loss: 1.0489269698155113e-05\n",
      "Epoch: 581, Loss: 1.0418530109745916e-05\n",
      "Epoch: 582, Loss: 1.0347408533561975e-05\n",
      "Epoch: 583, Loss: 1.0276793545926921e-05\n",
      "Epoch: 584, Loss: 1.020754461933393e-05\n",
      "Epoch: 585, Loss: 1.014054305414902e-05\n",
      "Epoch: 586, Loss: 1.0069898053188808e-05\n",
      "Epoch: 587, Loss: 1.0002395356423222e-05\n",
      "Epoch: 588, Loss: 9.935421985574067e-06\n",
      "Epoch: 589, Loss: 9.868695087789092e-06\n",
      "Epoch: 590, Loss: 9.802731256058905e-06\n",
      "Epoch: 591, Loss: 9.736386346048675e-06\n",
      "Epoch: 592, Loss: 9.670331564848311e-06\n",
      "Epoch: 593, Loss: 9.606121238903143e-06\n",
      "Epoch: 594, Loss: 9.541191502648871e-06\n",
      "Epoch: 595, Loss: 9.475783372181468e-06\n",
      "Epoch: 596, Loss: 9.4131128207664e-06\n",
      "Epoch: 597, Loss: 9.348545063403435e-06\n",
      "Epoch: 598, Loss: 9.286297427024692e-06\n",
      "Epoch: 599, Loss: 9.222771041095257e-06\n",
      "Epoch: 600, Loss: 9.160441550193354e-06\n",
      "Epoch: 601, Loss: 9.099800081457943e-06\n",
      "Epoch: 602, Loss: 9.038196367328055e-06\n",
      "Epoch: 603, Loss: 8.977382094599307e-06\n",
      "Epoch: 604, Loss: 8.916195838537533e-06\n",
      "Epoch: 605, Loss: 8.854931365931407e-06\n",
      "Epoch: 606, Loss: 8.795310350251384e-06\n",
      "Epoch: 607, Loss: 8.735890332900453e-06\n",
      "Epoch: 608, Loss: 8.676832294440828e-06\n",
      "Epoch: 609, Loss: 8.619207619631197e-06\n",
      "Epoch: 610, Loss: 8.560484275221825e-06\n",
      "Epoch: 611, Loss: 8.501583579345606e-06\n",
      "Epoch: 612, Loss: 8.444922059425153e-06\n",
      "Epoch: 613, Loss: 8.387435627810191e-06\n",
      "Epoch: 614, Loss: 8.331016942975111e-06\n",
      "Epoch: 615, Loss: 8.274789252027404e-06\n",
      "Epoch: 616, Loss: 8.218577931984328e-06\n",
      "Epoch: 617, Loss: 8.162731319316663e-06\n",
      "Epoch: 618, Loss: 8.108331712719519e-06\n",
      "Epoch: 619, Loss: 8.05383751867339e-06\n",
      "Epoch: 620, Loss: 8.000211892067455e-06\n",
      "Epoch: 621, Loss: 7.945674951770343e-06\n",
      "Epoch: 622, Loss: 7.891443601693027e-06\n",
      "Epoch: 623, Loss: 7.838243618607521e-06\n",
      "Epoch: 624, Loss: 7.785224624967668e-06\n",
      "Epoch: 625, Loss: 7.732460289844312e-06\n",
      "Epoch: 626, Loss: 7.679475857003126e-06\n",
      "Epoch: 627, Loss: 7.627662853337824e-06\n",
      "Epoch: 628, Loss: 7.576915322715649e-06\n",
      "Epoch: 629, Loss: 7.526336503360653e-06\n",
      "Epoch: 630, Loss: 7.475176062143873e-06\n",
      "Epoch: 631, Loss: 7.424881914630532e-06\n",
      "Epoch: 632, Loss: 7.373937933152774e-06\n",
      "Epoch: 633, Loss: 7.324672878894489e-06\n",
      "Epoch: 634, Loss: 7.2754264692775905e-06\n",
      "Epoch: 635, Loss: 7.225217814266216e-06\n",
      "Epoch: 636, Loss: 7.177764018706512e-06\n",
      "Epoch: 637, Loss: 7.129691766749602e-06\n",
      "Epoch: 638, Loss: 7.081252078933176e-06\n",
      "Epoch: 639, Loss: 7.033049769233912e-06\n",
      "Epoch: 640, Loss: 6.985704203543719e-06\n",
      "Epoch: 641, Loss: 6.938519618415739e-06\n",
      "Epoch: 642, Loss: 6.891495104355272e-06\n",
      "Epoch: 643, Loss: 6.845151801826432e-06\n",
      "Epoch: 644, Loss: 6.798193680879194e-06\n",
      "Epoch: 645, Loss: 6.751985438313568e-06\n",
      "Epoch: 646, Loss: 6.707467491651187e-06\n",
      "Epoch: 647, Loss: 6.661320185230579e-06\n",
      "Epoch: 648, Loss: 6.617102826567134e-06\n",
      "Epoch: 649, Loss: 6.572608072019648e-06\n",
      "Epoch: 650, Loss: 6.5274375629087444e-06\n",
      "Epoch: 651, Loss: 6.485158337454777e-06\n",
      "Epoch: 652, Loss: 6.441109690058511e-06\n",
      "Epoch: 653, Loss: 6.397630841092905e-06\n",
      "Epoch: 654, Loss: 6.353293429128826e-06\n",
      "Epoch: 655, Loss: 6.310924618446734e-06\n",
      "Epoch: 656, Loss: 6.267473509069532e-06\n",
      "Epoch: 657, Loss: 6.226130153663689e-06\n",
      "Epoch: 658, Loss: 6.184187895996729e-06\n",
      "Epoch: 659, Loss: 6.142387519503245e-06\n",
      "Epoch: 660, Loss: 6.1008636293991e-06\n",
      "Epoch: 661, Loss: 6.060513896954944e-06\n",
      "Epoch: 662, Loss: 6.019217835273594e-06\n",
      "Epoch: 663, Loss: 5.978080935165053e-06\n",
      "Epoch: 664, Loss: 5.937603418715298e-06\n",
      "Epoch: 665, Loss: 5.896946731809294e-06\n",
      "Epoch: 666, Loss: 5.85811130804359e-06\n",
      "Epoch: 667, Loss: 5.817430064780638e-06\n",
      "Epoch: 668, Loss: 5.780099399999017e-06\n",
      "Epoch: 669, Loss: 5.739690095651895e-06\n",
      "Epoch: 670, Loss: 5.701686859538313e-06\n",
      "Epoch: 671, Loss: 5.662322564603528e-06\n",
      "Epoch: 672, Loss: 5.62474178877892e-06\n",
      "Epoch: 673, Loss: 5.586879069596762e-06\n",
      "Epoch: 674, Loss: 5.549842171603814e-06\n",
      "Epoch: 675, Loss: 5.5115524446591735e-06\n",
      "Epoch: 676, Loss: 5.475006673805183e-06\n",
      "Epoch: 677, Loss: 5.437202162283938e-06\n",
      "Epoch: 678, Loss: 5.399914243753301e-06\n",
      "Epoch: 679, Loss: 5.364634944271529e-06\n",
      "Epoch: 680, Loss: 5.327994585968554e-06\n",
      "Epoch: 681, Loss: 5.291034085530555e-06\n",
      "Epoch: 682, Loss: 5.256647909845924e-06\n",
      "Epoch: 683, Loss: 5.220723778620595e-06\n",
      "Epoch: 684, Loss: 5.184702786209527e-06\n",
      "Epoch: 685, Loss: 5.14975909027271e-06\n",
      "Epoch: 686, Loss: 5.115835847391281e-06\n",
      "Epoch: 687, Loss: 5.082024472358171e-06\n",
      "Epoch: 688, Loss: 5.046886599302525e-06\n",
      "Epoch: 689, Loss: 5.012086603528587e-06\n",
      "Epoch: 690, Loss: 4.9786203817348e-06\n",
      "Epoch: 691, Loss: 4.945341061102226e-06\n",
      "Epoch: 692, Loss: 4.912740223517176e-06\n",
      "Epoch: 693, Loss: 4.878726940660272e-06\n",
      "Epoch: 694, Loss: 4.846846422879025e-06\n",
      "Epoch: 695, Loss: 4.814149178855587e-06\n",
      "Epoch: 696, Loss: 4.780915332958102e-06\n",
      "Epoch: 697, Loss: 4.748500941786915e-06\n",
      "Epoch: 698, Loss: 4.716629973700037e-06\n",
      "Epoch: 699, Loss: 4.684866325987969e-06\n",
      "Epoch: 700, Loss: 4.653209998650709e-06\n",
      "Epoch: 701, Loss: 4.621660536940908e-06\n",
      "Epoch: 702, Loss: 4.5906463128631e-06\n",
      "Epoch: 703, Loss: 4.56042607765994e-06\n",
      "Epoch: 704, Loss: 4.5296765165403485e-06\n",
      "Epoch: 705, Loss: 4.498140242503723e-06\n",
      "Epoch: 706, Loss: 4.467734015634051e-06\n",
      "Epoch: 707, Loss: 4.437312782101799e-06\n",
      "Epoch: 708, Loss: 4.407791948324302e-06\n",
      "Epoch: 709, Loss: 4.378010089567397e-06\n",
      "Epoch: 710, Loss: 4.3481272768985946e-06\n",
      "Epoch: 711, Loss: 4.3193895180593245e-06\n",
      "Epoch: 712, Loss: 4.288995114620775e-06\n",
      "Epoch: 713, Loss: 4.261662979843095e-06\n",
      "Epoch: 714, Loss: 4.231883394822944e-06\n",
      "Epoch: 715, Loss: 4.203532625979278e-06\n",
      "Epoch: 716, Loss: 4.175276899331948e-06\n",
      "Epoch: 717, Loss: 4.146766968915472e-06\n",
      "Epoch: 718, Loss: 4.118745891901199e-06\n",
      "Epoch: 719, Loss: 4.0907775655796286e-06\n",
      "Epoch: 720, Loss: 4.062723746756092e-06\n",
      "Epoch: 721, Loss: 4.036121936223935e-06\n",
      "Epoch: 722, Loss: 4.008835730928695e-06\n",
      "Epoch: 723, Loss: 3.981725512858247e-06\n",
      "Epoch: 724, Loss: 3.955308784497902e-06\n",
      "Epoch: 725, Loss: 3.9290630411414895e-06\n",
      "Epoch: 726, Loss: 3.901569471054245e-06\n",
      "Epoch: 727, Loss: 3.8753119042667095e-06\n",
      "Epoch: 728, Loss: 3.849156200885773e-06\n",
      "Epoch: 729, Loss: 3.822752660198603e-06\n",
      "Epoch: 730, Loss: 3.79798143512744e-06\n",
      "Epoch: 731, Loss: 3.771888714254601e-06\n",
      "Epoch: 732, Loss: 3.7460849853232503e-06\n",
      "Epoch: 733, Loss: 3.7210454593150644e-06\n",
      "Epoch: 734, Loss: 3.695799478009576e-06\n",
      "Epoch: 735, Loss: 3.6709293453895953e-06\n",
      "Epoch: 736, Loss: 3.6458550312090665e-06\n",
      "Epoch: 737, Loss: 3.62226705874491e-06\n",
      "Epoch: 738, Loss: 3.5972286696051015e-06\n",
      "Epoch: 739, Loss: 3.5728232887777267e-06\n",
      "Epoch: 740, Loss: 3.548307631717762e-06\n",
      "Epoch: 741, Loss: 3.5248062886239495e-06\n",
      "Epoch: 742, Loss: 3.5014461445825873e-06\n",
      "Epoch: 743, Loss: 3.4781016893248307e-06\n",
      "Epoch: 744, Loss: 3.454014859016752e-06\n",
      "Epoch: 745, Loss: 3.4307900023122784e-06\n",
      "Epoch: 746, Loss: 3.4077434065693524e-06\n",
      "Epoch: 747, Loss: 3.3847747999971034e-06\n",
      "Epoch: 748, Loss: 3.3621502097958e-06\n",
      "Epoch: 749, Loss: 3.3394239835615736e-06\n",
      "Epoch: 750, Loss: 3.3166870707646012e-06\n",
      "Epoch: 751, Loss: 3.294389898655936e-06\n",
      "Epoch: 752, Loss: 3.2727525649534073e-06\n",
      "Epoch: 753, Loss: 3.2502939575351775e-06\n",
      "Epoch: 754, Loss: 3.2282216579915257e-06\n",
      "Epoch: 755, Loss: 3.206384917575633e-06\n",
      "Epoch: 756, Loss: 3.1846241199673386e-06\n",
      "Epoch: 757, Loss: 3.1637052870792104e-06\n",
      "Epoch: 758, Loss: 3.1424910957866814e-06\n",
      "Epoch: 759, Loss: 3.1206066068989458e-06\n",
      "Epoch: 760, Loss: 3.0994901862868574e-06\n",
      "Epoch: 761, Loss: 3.079410134887439e-06\n",
      "Epoch: 762, Loss: 3.0580954444303643e-06\n",
      "Epoch: 763, Loss: 3.0379817417269805e-06\n",
      "Epoch: 764, Loss: 3.0169790079526138e-06\n",
      "Epoch: 765, Loss: 2.9971686217322713e-06\n",
      "Epoch: 766, Loss: 2.977042413476738e-06\n",
      "Epoch: 767, Loss: 2.956475100290845e-06\n",
      "Epoch: 768, Loss: 2.9365332920860965e-06\n",
      "Epoch: 769, Loss: 2.9171644655434648e-06\n",
      "Epoch: 770, Loss: 2.8974736778764054e-06\n",
      "Epoch: 771, Loss: 2.8771839879482286e-06\n",
      "Epoch: 772, Loss: 2.8578492674569134e-06\n",
      "Epoch: 773, Loss: 2.8387312340782955e-06\n",
      "Epoch: 774, Loss: 2.8195263439556584e-06\n",
      "Epoch: 775, Loss: 2.800537231451017e-06\n",
      "Epoch: 776, Loss: 2.7816477086162195e-06\n",
      "Epoch: 777, Loss: 2.7634253001451725e-06\n",
      "Epoch: 778, Loss: 2.744956873357296e-06\n",
      "Epoch: 779, Loss: 2.726072125369683e-06\n",
      "Epoch: 780, Loss: 2.7074011086369865e-06\n",
      "Epoch: 781, Loss: 2.6890422759606736e-06\n",
      "Epoch: 782, Loss: 2.6709135454439092e-06\n",
      "Epoch: 783, Loss: 2.652757757459767e-06\n",
      "Epoch: 784, Loss: 2.6347177026764257e-06\n",
      "Epoch: 785, Loss: 2.6180100576311816e-06\n",
      "Epoch: 786, Loss: 2.5995675514423056e-06\n",
      "Epoch: 787, Loss: 2.581656190159265e-06\n",
      "Epoch: 788, Loss: 2.5646970698289806e-06\n",
      "Epoch: 789, Loss: 2.5467625164310448e-06\n",
      "Epoch: 790, Loss: 2.5305432700406527e-06\n",
      "Epoch: 791, Loss: 2.5127183107542805e-06\n",
      "Epoch: 792, Loss: 2.496163233445259e-06\n",
      "Epoch: 793, Loss: 2.478903297742363e-06\n",
      "Epoch: 794, Loss: 2.4619198484288063e-06\n",
      "Epoch: 795, Loss: 2.4457699510094244e-06\n",
      "Epoch: 796, Loss: 2.4289008706546156e-06\n",
      "Epoch: 797, Loss: 2.4128601125994464e-06\n",
      "Epoch: 798, Loss: 2.396774107182864e-06\n",
      "Epoch: 799, Loss: 2.3800753297109623e-06\n",
      "Epoch: 800, Loss: 2.3641969164600596e-06\n",
      "Epoch: 801, Loss: 2.3491725187341217e-06\n",
      "Epoch: 802, Loss: 2.3329957912210375e-06\n",
      "Epoch: 803, Loss: 2.316751078978996e-06\n",
      "Epoch: 804, Loss: 2.3008569769444875e-06\n",
      "Epoch: 805, Loss: 2.2856697796669323e-06\n",
      "Epoch: 806, Loss: 2.270077857247088e-06\n",
      "Epoch: 807, Loss: 2.254475703011849e-06\n",
      "Epoch: 808, Loss: 2.239506102341693e-06\n",
      "Epoch: 809, Loss: 2.224523313998361e-06\n",
      "Epoch: 810, Loss: 2.209141712228302e-06\n",
      "Epoch: 811, Loss: 2.19445382754202e-06\n",
      "Epoch: 812, Loss: 2.1801006369059905e-06\n",
      "Epoch: 813, Loss: 2.165380010410445e-06\n",
      "Epoch: 814, Loss: 2.1506293705897406e-06\n",
      "Epoch: 815, Loss: 2.13594739761902e-06\n",
      "Epoch: 816, Loss: 2.121315901604248e-06\n",
      "Epoch: 817, Loss: 2.1067651232442586e-06\n",
      "Epoch: 818, Loss: 2.093339389830362e-06\n",
      "Epoch: 819, Loss: 2.0789154859812697e-06\n",
      "Epoch: 820, Loss: 2.064402679025079e-06\n",
      "Epoch: 821, Loss: 2.0505419797700597e-06\n",
      "Epoch: 822, Loss: 2.037297235801816e-06\n",
      "Epoch: 823, Loss: 2.023352180913207e-06\n",
      "Epoch: 824, Loss: 2.010371417782153e-06\n",
      "Epoch: 825, Loss: 1.995689672185108e-06\n",
      "Epoch: 826, Loss: 1.9830786186503246e-06\n",
      "Epoch: 827, Loss: 1.9693206922966056e-06\n",
      "Epoch: 828, Loss: 1.956032519956352e-06\n",
      "Epoch: 829, Loss: 1.942481731020962e-06\n",
      "Epoch: 830, Loss: 1.9295912352390587e-06\n",
      "Epoch: 831, Loss: 1.9167889604432276e-06\n",
      "Epoch: 832, Loss: 1.9044300643145107e-06\n",
      "Epoch: 833, Loss: 1.8905863043983118e-06\n",
      "Epoch: 834, Loss: 1.877869181043934e-06\n",
      "Epoch: 835, Loss: 1.865240847109817e-06\n",
      "Epoch: 836, Loss: 1.8522530353948241e-06\n",
      "Epoch: 837, Loss: 1.840897652982676e-06\n",
      "Epoch: 838, Loss: 1.8279946516486234e-06\n",
      "Epoch: 839, Loss: 1.8152211396227358e-06\n",
      "Epoch: 840, Loss: 1.8027603800874203e-06\n",
      "Epoch: 841, Loss: 1.7911630720846006e-06\n",
      "Epoch: 842, Loss: 1.778829755494371e-06\n",
      "Epoch: 843, Loss: 1.7669244698481634e-06\n",
      "Epoch: 844, Loss: 1.7550592019688338e-06\n",
      "Epoch: 845, Loss: 1.7432339518563822e-06\n",
      "Epoch: 846, Loss: 1.7314487195108086e-06\n",
      "Epoch: 847, Loss: 1.719703504932113e-06\n",
      "Epoch: 848, Loss: 1.7079983081202954e-06\n",
      "Epoch: 849, Loss: 1.6968875797829241e-06\n",
      "Epoch: 850, Loss: 1.6856874935911037e-06\n",
      "Epoch: 851, Loss: 1.674098712101113e-06\n",
      "Epoch: 852, Loss: 1.6625499483780004e-06\n",
      "Epoch: 853, Loss: 1.6514641174580902e-06\n",
      "Epoch: 854, Loss: 1.6405392671003938e-06\n",
      "Epoch: 855, Loss: 1.629106918699108e-06\n",
      "Epoch: 856, Loss: 1.618379201318021e-06\n",
      "Epoch: 857, Loss: 1.6073199731181376e-06\n",
      "Epoch: 858, Loss: 1.5962992847562418e-06\n",
      "Epoch: 859, Loss: 1.5857931430218741e-06\n",
      "Epoch: 860, Loss: 1.5744408301543444e-06\n",
      "Epoch: 861, Loss: 1.564626813888026e-06\n",
      "Epoch: 862, Loss: 1.5531359167653136e-06\n",
      "Epoch: 863, Loss: 1.5436365856658085e-06\n",
      "Epoch: 864, Loss: 1.5328362223954173e-06\n",
      "Epoch: 865, Loss: 1.522829734312836e-06\n",
      "Epoch: 866, Loss: 1.512053358965204e-06\n",
      "Epoch: 867, Loss: 1.5021742001408711e-06\n",
      "Epoch: 868, Loss: 1.4918996384949423e-06\n",
      "Epoch: 869, Loss: 1.482027983001899e-06\n",
      "Epoch: 870, Loss: 1.4715551515109837e-06\n",
      "Epoch: 871, Loss: 1.4620181900681928e-06\n",
      "Epoch: 872, Loss: 1.4521876892104046e-06\n",
      "Epoch: 873, Loss: 1.442687789676711e-06\n",
      "Epoch: 874, Loss: 1.4323552477435442e-06\n",
      "Epoch: 875, Loss: 1.4225352060748264e-06\n",
      "Epoch: 876, Loss: 1.413248355675023e-06\n",
      "Epoch: 877, Loss: 1.4038769222679548e-06\n",
      "Epoch: 878, Loss: 1.39453675274126e-06\n",
      "Epoch: 879, Loss: 1.385576410939393e-06\n",
      "Epoch: 880, Loss: 1.375722604279872e-06\n",
      "Epoch: 881, Loss: 1.3664766811416484e-06\n",
      "Epoch: 882, Loss: 1.3572620218837983e-06\n",
      "Epoch: 883, Loss: 1.347510419691389e-06\n",
      "Epoch: 884, Loss: 1.3387025319389068e-06\n",
      "Epoch: 885, Loss: 1.3295821190695278e-06\n",
      "Epoch: 886, Loss: 1.3205320783526986e-06\n",
      "Epoch: 887, Loss: 1.312372432948905e-06\n",
      "Epoch: 888, Loss: 1.3033813957008533e-06\n",
      "Epoch: 889, Loss: 1.2944295804118156e-06\n",
      "Epoch: 890, Loss: 1.285851794818882e-06\n",
      "Epoch: 891, Loss: 1.2773509752150858e-06\n",
      "Epoch: 892, Loss: 1.2683398153967573e-06\n",
      "Epoch: 893, Loss: 1.2596017313626362e-06\n",
      "Epoch: 894, Loss: 1.2519958545453846e-06\n",
      "Epoch: 895, Loss: 1.2428527043084614e-06\n",
      "Epoch: 896, Loss: 1.2345924460532842e-06\n",
      "Epoch: 897, Loss: 1.2265736586414278e-06\n",
      "Epoch: 898, Loss: 1.218491320287285e-06\n",
      "Epoch: 899, Loss: 1.2095715646864846e-06\n",
      "Epoch: 900, Loss: 1.2017768540317775e-06\n",
      "Epoch: 901, Loss: 1.19351261673728e-06\n",
      "Epoch: 902, Loss: 1.1861451412187307e-06\n",
      "Epoch: 903, Loss: 1.1775609891628847e-06\n",
      "Epoch: 904, Loss: 1.1698703019646928e-06\n",
      "Epoch: 905, Loss: 1.1617166819632985e-06\n",
      "Epoch: 906, Loss: 1.1544482276804047e-06\n",
      "Epoch: 907, Loss: 1.1459796951385215e-06\n",
      "Epoch: 908, Loss: 1.1383930313968449e-06\n",
      "Epoch: 909, Loss: 1.1305971838737605e-06\n",
      "Epoch: 910, Loss: 1.1231355756535777e-06\n",
      "Epoch: 911, Loss: 1.11562530946685e-06\n",
      "Epoch: 912, Loss: 1.108140509131772e-06\n",
      "Epoch: 913, Loss: 1.1007530247297836e-06\n",
      "Epoch: 914, Loss: 1.0930885991911055e-06\n",
      "Epoch: 915, Loss: 1.0857729648705572e-06\n",
      "Epoch: 916, Loss: 1.0781686796690337e-06\n",
      "Epoch: 917, Loss: 1.070767439159681e-06\n",
      "Epoch: 918, Loss: 1.0640881100698607e-06\n",
      "Epoch: 919, Loss: 1.0568346624495462e-06\n",
      "Epoch: 920, Loss: 1.0501989891054109e-06\n",
      "Epoch: 921, Loss: 1.0424525953567354e-06\n",
      "Epoch: 922, Loss: 1.0355342965340242e-06\n",
      "Epoch: 923, Loss: 1.0282803941663587e-06\n",
      "Epoch: 924, Loss: 1.0217356702924008e-06\n",
      "Epoch: 925, Loss: 1.014919689623639e-06\n",
      "Epoch: 926, Loss: 1.008418621495366e-06\n",
      "Epoch: 927, Loss: 1.0012596476371982e-06\n",
      "Epoch: 928, Loss: 9.94512447505258e-07\n",
      "Epoch: 929, Loss: 9.877882121145376e-07\n",
      "Epoch: 930, Loss: 9.812304142542416e-07\n",
      "Epoch: 931, Loss: 9.745990610099398e-07\n",
      "Epoch: 932, Loss: 9.679424692876637e-07\n",
      "Epoch: 933, Loss: 9.613088423066074e-07\n",
      "Epoch: 934, Loss: 9.5474513273075e-07\n",
      "Epoch: 935, Loss: 9.481098572905466e-07\n",
      "Epoch: 936, Loss: 9.421456752534141e-07\n",
      "Epoch: 937, Loss: 9.360128387925215e-07\n",
      "Epoch: 938, Loss: 9.291108540310233e-07\n",
      "Epoch: 939, Loss: 9.230822115569026e-07\n",
      "Epoch: 940, Loss: 9.168267638415273e-07\n",
      "Epoch: 941, Loss: 9.111463441513479e-07\n",
      "Epoch: 942, Loss: 9.044806574820541e-07\n",
      "Epoch: 943, Loss: 8.980229040389531e-07\n",
      "Epoch: 944, Loss: 8.922183951654006e-07\n",
      "Epoch: 945, Loss: 8.861603646437288e-07\n",
      "Epoch: 946, Loss: 8.800265050012968e-07\n",
      "Epoch: 947, Loss: 8.744154911255464e-07\n",
      "Epoch: 948, Loss: 8.68072675075382e-07\n",
      "Epoch: 949, Loss: 8.623592293588445e-07\n",
      "Epoch: 950, Loss: 8.568500788896927e-07\n",
      "Epoch: 951, Loss: 8.507931852363981e-07\n",
      "Epoch: 952, Loss: 8.455353963654488e-07\n",
      "Epoch: 953, Loss: 8.39336109947908e-07\n",
      "Epoch: 954, Loss: 8.340833801412373e-07\n",
      "Epoch: 955, Loss: 8.281820669253648e-07\n",
      "Epoch: 956, Loss: 8.230328489844396e-07\n",
      "Epoch: 957, Loss: 8.171406307155848e-07\n",
      "Epoch: 958, Loss: 8.120263146338402e-07\n",
      "Epoch: 959, Loss: 8.05894160293974e-07\n",
      "Epoch: 960, Loss: 8.00747329776641e-07\n",
      "Epoch: 961, Loss: 7.950027907099866e-07\n",
      "Epoch: 962, Loss: 7.898914304860227e-07\n",
      "Epoch: 963, Loss: 7.846487051210715e-07\n",
      "Epoch: 964, Loss: 7.791383040967048e-07\n",
      "Epoch: 965, Loss: 7.739932925687754e-07\n",
      "Epoch: 966, Loss: 7.690634333812341e-07\n",
      "Epoch: 967, Loss: 7.638903980478062e-07\n",
      "Epoch: 968, Loss: 7.584534955640265e-07\n",
      "Epoch: 969, Loss: 7.532931931564235e-07\n",
      "Epoch: 970, Loss: 7.485137984986068e-07\n",
      "Epoch: 971, Loss: 7.434104531967023e-07\n",
      "Epoch: 972, Loss: 7.380469924100908e-07\n",
      "Epoch: 973, Loss: 7.331502729357453e-07\n",
      "Epoch: 974, Loss: 7.282699243660318e-07\n",
      "Epoch: 975, Loss: 7.234059467009502e-07\n",
      "Epoch: 976, Loss: 7.185583399405004e-07\n",
      "Epoch: 977, Loss: 7.137271040846827e-07\n",
      "Epoch: 978, Loss: 7.089122391334968e-07\n",
      "Epoch: 979, Loss: 7.041137450869428e-07\n",
      "Epoch: 980, Loss: 6.993316219450207e-07\n",
      "Epoch: 981, Loss: 6.945658697077306e-07\n",
      "Epoch: 982, Loss: 6.900733069414855e-07\n",
      "Epoch: 983, Loss: 6.853389891148254e-07\n",
      "Epoch: 984, Loss: 6.80621099036216e-07\n",
      "Epoch: 985, Loss: 6.761175086467119e-07\n",
      "Epoch: 986, Loss: 6.718597660437808e-07\n",
      "Epoch: 987, Loss: 6.6702568801702e-07\n",
      "Epoch: 988, Loss: 6.624443358305143e-07\n",
      "Epoch: 989, Loss: 6.582299079127552e-07\n",
      "Epoch: 990, Loss: 6.538013508361473e-07\n",
      "Epoch: 991, Loss: 6.496267701550096e-07\n",
      "Epoch: 992, Loss: 6.4487795725654e-07\n",
      "Epoch: 993, Loss: 6.405334715964273e-07\n",
      "Epoch: 994, Loss: 6.35972355667036e-07\n",
      "Epoch: 995, Loss: 6.316844860521087e-07\n",
      "Epoch: 996, Loss: 6.280335469455167e-07\n",
      "Epoch: 997, Loss: 6.235171667867689e-07\n",
      "Epoch: 998, Loss: 6.192458386067301e-07\n",
      "Epoch: 999, Loss: 6.15353201283142e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[15.0012],\n",
       "        [17.0017]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model_01.train()\n",
    "    y_preds = model_01(X_train)\n",
    "    loss = loss_fn(y_preds, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))\n",
    "# Test the network\n",
    "test_data = torch.tensor([[6, 1], [7, 1]], dtype=torch.float32)\n",
    "predictions = model_01(test_data)\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
